Refer to ./data/description.md for task overview and evaluation details. Data files are in ./data.

This is an iterative improvement loop. Do NOT stop just because a file says the task is completed. Treat prior "completed" states as snapshots; your job each run is to improve, analyze, or tighten the solution.

General principles (keep it simple and competition‑agnostic):
- Favor straightforward, strong baselines suitable for the task and data size.
- Keep compute modest. Prefer small models/searches and short runs; use GPU if available but do not depend on it.
- Avoid heavy tooling (e.g., no Optuna). Use small `RandomizedSearchCV` or tiny grids when tuning.

Iterative improvement rules (override any "task completed" status):
- Use multiple seeds when models are stochastic. Default to 3 seeds (e.g., {42, 123, 456}); increase if results look noisy. Report mean±std and 95% CI.
- Aim for 2–3 well‑motivated experiments per run, budget permitting.
- If multiple consecutive runs with solid experiments show no meaningful improvement, you may declare a plateau and propose next steps.
- Primary metric: use the competition’s official metric (parse from metadata or `data/description.md` where possible). Also report one relevant secondary metric when helpful. Decide by the primary metric.

Reproducibility and CV:
- Choose CV by task:
  - Classification: `StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`.
  - Regression: `KFold(n_splits=5, shuffle=True, random_state=42)`.
  - Groups: `GroupKFold(n_splits=5)` (or `StratifiedGroupKFold` if applicable).
  - Time series: `TimeSeriesSplit` with no shuffle; use a gap if leakage is likely.
- Keep folds fixed across experiments; vary only model seeds for stability. Log per‑fold scores and fit times.

Algorithms and features (examples, not mandates):
- Tabular: Gradient boosting (LightGBM/XGBoost/CatBoost if available) or scikit‑learn baselines (LogisticRegression/Linear models/RandomForest).
- NLP: TF‑IDF with linear models or MultinomialNB; consider mixing word/char n‑grams.
- Vision: transfer learning with pretrained backbones; short fine‑tunes with early stopping if GPU.
- Time series/forecast: simple lag/rolling features with boosting or linear baselines; classical models if available.
- Keep feature additions minimal; retain only those that help in ablation. Revert changes that don’t move the primary metric.

Search strategy:
- Prefer small `RandomizedSearchCV` or tiny grids over wide searches.
- Tune impactful parameters first (e.g., regularization strength, learning_rate, max_depth, n_estimators, n‑gram ranges, max_features) within a modest time budget.
- Only consider simple ensembles after a single model shows a stable win over baseline.

Decision and shipping:
- Update `submission.csv` only if the new approach clearly and repeatably outperforms the stability‑checked baseline, with evidence (e.g., non‑overlapping 95% CIs or similar) and a brief rationale for practical significance on this metric. Avoid updating on noise.
- Validate `submission.csv` against `data/sample_submission.csv` if present: columns/order/shape/dtypes; no NaNs/±inf; deterministic row ordering by ID.
- If the previous `submission.csv` is broken, regenerate it using the best current model.
 - Keep a versioned copy when shipping: write to `submissions/submission_{iter:03d}.csv` (or timestamped if no iter id), and also write/update `submission.csv` as the latest. Include the file hash in the journal entry.

Experiment tracking:
- Append rows to `experiments.csv`: id, description, task_type, model_family, params, features, metric_name, higher_is_better, fold_scheme, cv_mean, cv_std, ci_low, ci_high, fit_time_s, seed(s).
- Include a short Self‑critique (≥2 risks) and brief error analysis tailored to the task (e.g., top confusions for classification; residuals for regression; horizon buckets for forecasting).

Execution guidance:
- Each training run should have a timeout, to prevent it running too long. Default 1 hour. You can adjust as needed.
- Use project scripts if provided (e.g., `./prompt.sh`, `./run_iterations.sh`). Otherwise, run Python entrypoints directly.
- Keep random seeds explicit and consistent across experiments; document them in results.

Required behavior every run:
1) Context load
   - Read `agent_status.md` (if present), the latest 2–3 `journal/journal_*.md` (if present), review `./data/description.md`, list files in `data/`, and read `data/sample_submission.csv` if available.
   - Parse the last best cross‑validation metric and approach from prior journals. Call this `previous_best`.
   - Review `requirements.txt` for available packages and prefer those already installed.

2) Set a concrete goal for THIS run
   - Target: beat `previous_best` on the primary metric in a statistically and practically meaningful way; otherwise complete at least 2–3 meaningful experiments with proper evaluation and analysis.
   - Declare the time/compute budget you will use this run and keep experiments within it.

3) Plan before doing
   - Write/append `model_prototype_plan.md` with 3–5 hypotheses, expected impact, and time cost. Prefer fast, high‑signal experiments first.

4) Execute experiments iteratively (minimum 2–3 unless you surpass the target early)
   - Use the CV policy above; log per‑fold scores and fit times; report the primary metric (and one secondary if helpful), mean±std, and 95% CI.
   - Record each experiment per "Experiment tracking".
   - Avoid test leakage; do not fit on test when estimating CV. Use quick subsamples for sanity checks if needed.

5) Decide and ship
   - Select the best performing approach from this run. Regenerate `submission.csv` from the full train fit ONLY if it beats `previous_best` per the decision rule; otherwise do not update `submission.csv`.
   - If no improvement, perform a short error analysis and propose next steps.

6) Logging and status
   - Create `journal/journal_YYYYMMDD_HHMMSS.md` with sections: Context, Goal, Plan, Experiments (table), Results, Self‑critique (≥2 risks/weaknesses), Decision, Next actions.
   - Update `agent_status.md` with current best score, best approach, and a prioritized TODO backlog for the next run. Do NOT set the status to "completed" unless termination criteria below are met.
   - If shipped, note the new `submission.csv` and delta vs `previous_best`.

Termination criteria (when you may mark completed in `agent_status.md`):
   - Either: improved over `previous_best` in a statistically and practically meaningful way and there are no low‑cost, high‑likelihood next steps remaining within budget; or
   - After multiple consecutive runs with solid experiments and no measurable improvement, mark as "plateaued", summarize why, and propose higher‑effort strategies.

Safety and quality guardrails:
   - Never read or infer labels for test data; use test only for final predictions.
   - Keep preprocessing identical across CV folds and final fit.
   - Always report fold stats and random_state used.

Housekeeping:
   - If `eda_plan.md` does not exist, create it with a concise data exploration plan; otherwise, update if new insights emerge.
   - Maintain `model_prototype_plan.md` as a living plan; update it each run with what was tried and what is next.
   - You may install Python packages, edit code, and run code. Summarize results in a new time‑stamped journal entry each run.

Protected files (DO NOT MODIFY):
   - `user_prompt.txt` — This orchestration prompt
   - `prompt.sh` — Agent execution script
   - `run_iterations.sh` — Multi-iteration convenience script
   - `requirements.txt` — Core dependencies (only modify if absolutely necessary for ML experiments)
   - `setup_env.sh` — Environment setup script
